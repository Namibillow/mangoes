{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cleared-style",
   "metadata": {},
   "source": [
    "## Building dependency based co-occurrence matrix \n",
    "\n",
    "This is a compiled examples of how to build co-occurrence matrix using dependency relations.\n",
    "\n",
    "### Note\n",
    "- Fixed the lowercase issue for CONLLU \n",
    "- If CONLLU is the format used for the corpus and the POS is missing, then instead xpos (if available) will be replaced as POS. \n",
    "- Analogies currently does not support Token type. Only works if one field (in this case \"form\" or \"lemma\") is used for target_vocabulary. \n",
    "- Even if parameter deprel for dependency_context does not list dependency relation denoting prepositions such as \"prep\" or \"case\" will be included. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import mangoes\n",
    "import nltk\n",
    "import string \n",
    "import pprint \n",
    "import os \n",
    "import datetime \n",
    "import logging \n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-coverage",
   "metadata": {},
   "source": [
    "Read more for \n",
    "\n",
    "https://github.com/UniversalDependencies/UD_English-EWT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or specify date to saved ones\n",
    "# eg: date = \"2021-03-24-15-23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = os.path.join(os.path.abspath(''), \"output/{}\".format(date))\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    print(\"made a dir: \", OUTPUT_PATH)\n",
    "    os.makedirs(OUTPUT_PATH)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for logging \n",
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    filename=f\"{OUTPUT_PATH}/log\", \n",
    "                    filemode=\"a+\",\n",
    "                    format=\"%(asctime)s;%(levelname)s;%(message)s\",\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-london",
   "metadata": {},
   "source": [
    "# Build Embedding \n",
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the data contained directory/file \n",
    "UD_English_EWT = \"../../UD_English-EWT-master/corpus\"\n",
    "# UD_English_EWT =  \"../../UD_English-EWT-master/corpus/en_ewt-ud-train.conllu\"\n",
    "# WIKI = \"../../wikipedia_data/treebank.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Corpus path: {UD_English_EWT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-contrary",
   "metadata": {},
   "source": [
    "NOTE: When loading dataset to `Corpus`, set `ignore_punctuation` to `False` if `DependencyContext` is used to build co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = mangoes.Corpus(UD_English_EWT, \n",
    "                        reader=mangoes.corpus.CONLLU, \n",
    "                        language=\"English\",\n",
    "                        lower=True, \n",
    "                        ignore_punctuation=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Corpus has {} sentences, {} words, {} unique tokens\".format(corpus.nb_sentences, corpus.size, len(corpus.words_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-appendix",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save corpus \n",
    "corpus_metadata = os.path.join(OUTPUT_PATH, \".corpus\")\n",
    "print(\"Saved Corpus\")\n",
    "corpus.save_metadata(corpus_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or load \n",
    "# corpus = mangoes.Corpus.load_from_metadata(corpus_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-conviction",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "#### -- Target Words --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Building target words...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_filter_lemma = mangoes.corpus.remove_elements(nltk.corpus.stopwords.words('english'), attribute=\"lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_filter = mangoes.corpus.remove_elements(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-burns",
   "metadata": {},
   "source": [
    "#### Option 1: Use Lemma and POS as targetwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocabulary = corpus.create_vocabulary(attributes=(\"lemma\",\"POS\"), \n",
    "                                              filters = [ stopwords_filter_lemma,  # removes stopwords \n",
    "                                                         mangoes.corpus.remove_most_frequent(100), # removes words that are frequent more than 100\n",
    "                                                         mangoes.corpus.remove_least_frequent(2)]) # removes words that are frequent less than 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-legislature",
   "metadata": {},
   "source": [
    "#### Option 2: Only keep lemma as targetwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocabulary = corpus.create_vocabulary(attributes=\"lemma\", \n",
    "                                              filters = [ stopwords_filter, \n",
    "                                                         mangoes.corpus.remove_most_frequent(100),\n",
    "                                                         mangoes.corpus.remove_least_frequent(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-athletics",
   "metadata": {},
   "source": [
    "#### Option 3: Use Lemma and POS as targetwords + keep only POS of \"NOUN\", \"VERB\", \"ADJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_filter_target = mangoes.corpus.filter_by_attribute(\"POS\", [\"NOUN\", \"VERB\", \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must include POS as one of attributes if POS filter is used\n",
    "target_vocabulary = corpus.create_vocabulary(attributes=(\"lemma\",\"POS\"), \n",
    "                                            filters = [ stopwords_filter_lemma, \n",
    "                                                        pos_filter_target,\n",
    "                                                        mangoes.corpus.remove_most_frequent(100),\n",
    "                                                        mangoes.corpus.remove_least_frequent(2)],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-queens",
   "metadata": {},
   "source": [
    "#### Check target vocaulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(target_vocabulary)} words will be used as target vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Target Words: {len(target_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-hudson",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check all the target vocabulary\n",
    "target_vocabulary._index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique counts for POS if POS is in target vocabulary \n",
    "\n",
    "if \"POS\" in target_vocabulary.entity:\n",
    "    POS_target_count = Counter(getattr(token, \"POS\") for token in target_vocabulary._index_word)\n",
    "    print(POS_target_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-dream",
   "metadata": {},
   "source": [
    "#### -- Context Words --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Building Context words...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-slope",
   "metadata": {},
   "source": [
    "#### Option 1: Only keep lemma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vocabulary = corpus.create_vocabulary(attributes=(\"lemma\"), \n",
    "                                              filters = [ stopwords_filter])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-cleanup",
   "metadata": {},
   "source": [
    "#### Option 2: Keep lemma and POS with \"VERB\" or \"ADJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_filter_context = mangoes.corpus.filter_by_attribute(\"POS\",[\"NOUN\", \"VERB\", \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vocabulary = corpus.create_vocabulary(attributes=(\"lemma\",\"POS\"), \n",
    "                                            filters = [ stopwords_filter_lemma,\n",
    "                                                        pos_filter_context])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-saver",
   "metadata": {},
   "source": [
    "#### Check context vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-expansion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"{len(context_vocabulary)} words will be used as target vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Context words: {len(context_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vocabulary._index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"POS\" in context_vocabulary.entity:\n",
    "    POS_context_count = Counter(getattr(token, \"POS\") for token in context_vocabulary._index_word)\n",
    "    print(POS_context_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-prediction",
   "metadata": {},
   "source": [
    "### Context "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-vinyl",
   "metadata": {},
   "source": [
    "#### Option 1: consider dependency relation  as undirected by setting directed parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: entity needs to match with context_vocabulary \n",
    "dependency_context = mangoes.context.DependencyBasedContext(entity=(\"lemma\", \"POS\"), \n",
    "                                                            labels=True,\n",
    "                                                            collapse=True, \n",
    "                                                            vocabulary=context_vocabulary,\n",
    "                                                            directed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-albuquerque",
   "metadata": {},
   "source": [
    "#### Option 2: consider only dependency relation of \"advmod\" or \"nsubj\"\n",
    "Note if collapse is set to True, then case will be included as well. \n",
    "\n",
    "\n",
    "This will result in changed number of context words when co-occurrence matrix is built "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_context = mangoes.context.DependencyBasedContext(entity=(\"lemma\"), \n",
    "                                                            labels=False,\n",
    "                                                            collapse=True, \n",
    "                                                            vocabulary=context_vocabulary,\n",
    "                                                            deprel_keep = (\"advmod\", \"nsubj\"),\n",
    "                                                            directed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-enemy",
   "metadata": {},
   "source": [
    "#### Option 3 adding depth to consider indirected dependency relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_context = mangoes.context.DependencyBasedContext(entity=(\"lemma\",\"POS\"), \n",
    "                                                            labels=True,\n",
    "                                                            collapse=True, \n",
    "                                                            vocabulary=context_vocabulary,\n",
    "                                                            directed=False,\n",
    "                                                            depth=2\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-absorption",
   "metadata": {},
   "source": [
    "#### Option 4: Combining option 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-sociology",
   "metadata": {},
   "source": [
    "If `depth` > 1 and `deprel_keep` is specified then `deprel_keep` will be counted as long as indirected dependency relations has one relation from `deprel_keep`.\n",
    "\n",
    "\n",
    "eg. If `depth=2` and `deprel_keep=(\"nsubj\",\"amod\")` then  `\"{'lemma': 'muslim', 'POS': 'ADJ'}/nsubj_amod\",` will be included as context words since one of the dependency relation `nsubj` and `amod` is in `deprel_keep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_context = mangoes.context.DependencyBasedContext(entity=(\"lemma\",\"POS\"), \n",
    "                                                            labels=True,\n",
    "                                                            collapse=True, \n",
    "                                                            vocabulary=context_vocabulary,\n",
    "                                                            directed=False,\n",
    "                                                            depth=2,\n",
    "                                                            deprel_keep=(\"advmod\", \"nsubj\")\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-shuttle",
   "metadata": {},
   "source": [
    "#### Option 5: Basis mapping \n",
    "\n",
    "So far all the previous options passes `label` as `True` thus, co-occurrence matrix context words considers unique token + dependency relation. However, we can apply basis mapping, meaning decouple the observed syntactic context from the final co-occurrence matrix. By setting the `label` to `False` while defining the context by using the DependnencyBasedContext, we still have all those constrains specified by other arguments but we will only count the unique tokens. \n",
    "\n",
    "e.g.: if `label=False` and `depth=2` then `{'lemma':'apple', 'POS':'NOUN'}/obj` and `{'lemma':'apple', 'POS':'NOUN'}/obj_mod` will mapped to context vocabulary of `{'lemma':'apple', 'POS':'NOUN'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_context = mangoes.context.DependencyBasedContext(entity=(\"lemma\",\"POS\"), \n",
    "                                                            labels=False,\n",
    "                                                            collapse=False, \n",
    "                                                            vocabulary=context_vocabulary,\n",
    "                                                            directed=False,\n",
    "                                                            depth=3                                                          \n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-cruise",
   "metadata": {},
   "source": [
    "#### Option 6: Weight\n",
    "\n",
    "Instead of counting the frequency and equally adding 1, you can also diffierentiate the relative importance of different paths. \n",
    "\n",
    "By setting `weight=True`, paths can weigh their respective contributions differently. It assigns a value of 1 to paths of length 1 and fractions to longer paths: 1 / len(path_lengths)\n",
    "\n",
    "Instead, if you want to use a more linguistically-informed path value function you can pass a dictionary to `weight_scheme` parameters. It will assign specific weight to grammatical relations, without considering their length:. \n",
    "e.g.: `weight_scheme={\"nsubj\":5}` means any paths that have `nsubj` as one of their dependency relations will get value of 5.  \n",
    "\n",
    "\n",
    "If the `weight` is not set but `weight_scheme` is then it will internally assign `weight=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_context = mangoes.context.DependencyBasedContext(entity=(\"lemma\",\"POS\"), \n",
    "                                                            labels=True,\n",
    "                                                            collapse=True, \n",
    "                                                            vocabulary=context_vocabulary,\n",
    "                                                            directed=False,\n",
    "                                                            depth=2, \n",
    "                                                            weight=True,\n",
    "                                                            weight_scheme={\"nsubj\":5, \"amod\":4, \"nmod\":3}\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save vocabularies \n",
    "target_vocabulary_file_name = \"vocabulary_{}_target_words\".format(len(target_vocabulary))\n",
    "context_vocabulary_file_name = \"vocabulary_{}_context_words\".format(len(context_vocabulary))\n",
    "\n",
    "# # or load by specify saved vocabulary files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-extra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # save\n",
    "target_vocabulary.save(OUTPUT_PATH, name=target_vocabulary_file_name)\n",
    "context_vocabulary.save(OUTPUT_PATH, name=context_vocabulary_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabs\n",
    "# target_vocabulary = mangoes.Vocabulary.load(\"output\", target_vocabulary_file_name)\n",
    "# context_vocabulary = mangoes.Vocabulary.load(\"output\", context_vocabulary_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-williams",
   "metadata": {},
   "source": [
    "### Cooccurrence Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-attempt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coocc_count = mangoes.counting.count_cooccurrence(corpus,  \n",
    "                                                target_vocabulary, \n",
    "                                                context=dependency_context,\n",
    "                                                )\n",
    "\n",
    "coocc_count.pprint(display=display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target words\")\n",
    "coocc_count.words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"context words\")\n",
    "coocc_count.contexts_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-tampa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coocc_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-discovery",
   "metadata": {},
   "source": [
    "#### Get closes words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-connection",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 8\n",
    "\n",
    "print(\"target word: \", target_vocabulary[i])\n",
    "print(\"close words\")\n",
    "\n",
    "# NOTE: make sure POS you want to get exist in target_vocabulary.  \n",
    "# only_verb = [x for x in coocc_count.get_closest_words(target_vocabulary[i], nb=100) if x[0].POS in ['VERB']]\n",
    "# pprint.pprint(only_verb)\n",
    "\n",
    "pprint.pprint(coocc_count.get_closest_words(target_vocabulary[i], 30)) # default nb is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique Dependency relation in built co-occurrence matrix\n",
    "# Run below only if labels=True was used\n",
    "if dependency_context.labels:\n",
    "    dep_rel_count = Counter(word.split(\"/\")[-1] for word in coocc_count.contexts_words)\n",
    "    pprint.pprint(dep_rel_count.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-flight",
   "metadata": {},
   "source": [
    "### Weighting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi = mangoes.weighting.PPMI()\n",
    "svd = mangoes.reduction.SVD(dimensions=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = mangoes.create_representation(coocc_count, weighting=ppmi, reduction=svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-photographer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embeddings.pprint(display=display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or you need to \n",
    "i = 100\n",
    "\n",
    "print(\"target word: \", target_vocabulary[i])\n",
    "print(\"close words\")\n",
    "\n",
    "only_verb = [x for x in embeddings.get_closest_words(target_vocabulary[i], nb=1000) if x[0].POS in ['VERB']]\n",
    "pprint.pprint(only_verb)\n",
    "\n",
    "# pprint.pprint(embeddings.get_closest_words(target_vocabulary[i], 30)) # default nb is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the embeddings \n",
    "embedding_path = os.path.join(OUTPUT_PATH,\n",
    "                              \"embeddings/ppmi_svd_{}target_words_deprel\".format(len(target_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.save(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding \n",
    "# embeddings = mangoes.Embeddings.load(embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-humanitarian",
   "metadata": {},
   "source": [
    "# Explore Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-edinburgh",
   "metadata": {},
   "source": [
    "### Closest Words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-fault",
   "metadata": {},
   "source": [
    "#### Similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = {0: \"cityblock\",\n",
    "        1: \"cosine\", \n",
    "        2: \"euclidean\", \n",
    "        3: \"l1\", \n",
    "        4:\"l2\", \n",
    "        5:\"manhattan\",\n",
    "        6:\"braycurtis\", \n",
    "        7:\"canberra\", \n",
    "        8:\"chebyshev\", \n",
    "        9:\"correlation\", \n",
    "        10:\"dice\", \n",
    "        11:\"hamming\", \n",
    "        12:\"jaccard\", \n",
    "        13:\"kulsinski\", \n",
    "        14:\"mahalanobis\", \n",
    "       15: \"minkowski\", \n",
    "       16: \"rogerstanimoto\", \n",
    "       17: \"russellrao\", \n",
    "       18: \"seuclidean\", \n",
    "        19:\"sokalmichener\", \n",
    "        20:\"sokalsneath\", \n",
    "       21: \"sqeuclidean\", \n",
    "       22: \"yule\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "nb = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-israel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = {word: pd.Series([w for w, _ in embeddings.get_closest_words(word, nb=nb, metric=sims[i])], index=range(1,nb+1))\n",
    "          for word in embeddings.words[100:200:10]}\n",
    "result_df = pd.DataFrame(result).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"similarity measure: {sims[i]}\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-diploma",
   "metadata": {},
   "source": [
    "### Analogies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only works if target vocabulary is not type Token. \n",
    "question = \"king queen male\"\n",
    "answer = \"female\"\n",
    "\n",
    "# If target vocabulary is a Token then uncomment following instead\n",
    "# from mangoes.utils.reader import AnnotatedSentenceGenerator \n",
    "# Token = AnnotatedSentenceGenerator.Token\n",
    "\n",
    "# question = [\n",
    "#     Token(lemma=\"queen\", POS=\"NOUN\"),\n",
    "#     Token(lemma=\"king\", POS=\"NOUN\"),\n",
    "#     Token(lemma=\"male\", POS=\"NOUN\")\n",
    "# ]\n",
    "# answer = Token(lemma=\"female\", POS=\"NOUN\")\n",
    "\n",
    "# filter_token = mangoes.vocabulary.create_token_filter([\"lemma\", \"POS\"])\n",
    "\n",
    "# question = list(map(filter_token, question))\n",
    "# answer = filter_token(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocation 2: Oper1\n",
    "question = \"help give help\"\n",
    "answer = \"offer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what words are available in our embeddings #\n",
    "for w in (question.split() + [answer]):\n",
    "    try:\n",
    "        embeddings.words.word_index[w]\n",
    "        print(f\"'{w}' exists\")\n",
    "    except KeyError:\n",
    "        print(f\"'{w}' doesn't exist in embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-wrapping",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can resolve analogy according to a representation using the analogy() method\n",
    "# Here, we will display the results of some examples :\n",
    "for analogy in [question]:\n",
    "    print(analogy, '->', embeddings.analogy(analogy,5).using_cosadd)\n",
    "    print(analogy, '->', embeddings.analogy(analogy,5).using_cosmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-surveillance",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import mangoes.visualize\n",
    "plt.figure()\n",
    "\n",
    "# 1. distances between the words\n",
    "ax = plt.subplot(221, projection='polar')\n",
    "mangoes.visualize.plot_distances(embeddings, ax)\n",
    "\n",
    "# 2. isotropy\n",
    "ax = plt.subplot(222)\n",
    "mangoes.visualize.plot_isotropy(embeddings, ax)\n",
    "\n",
    "# 3. t-sne\n",
    "plt.subplot(212)\n",
    "mangoes.visualize.plot_tsne(embeddings)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-birth",
   "metadata": {},
   "source": [
    "# Evalutaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-dealer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Evaluate\n",
    "import mangoes.evaluation.analogy\n",
    "\n",
    "google_dataset = mangoes.evaluation.analogy.GOOGLE\n",
    "msr_dataset = mangoes.evaluation.analogy.MSR\n",
    "\n",
    "analogy_evaluation = mangoes.evaluation.analogy.Evaluation(embeddings, google_dataset, msr_dataset)\n",
    "print(analogy_evaluation.get_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-guitar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
