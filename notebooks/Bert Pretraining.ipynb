{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from mangoes.modeling import BERTWordPieceTokenizer, BERTForMaskedLanguageModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can pretrain BERT on a corpus using the masked language model and next sentence prediction pretraining procedure from the original paper, or just using the masked language model objective.\n",
    "In this example, we will just use the MLM objective, but the code would look about the same if using NSP as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train a BERT model from scratch, we can train a subword tokenizer on our corpus (setting tokenizer parameters in the initialization function call), then save it to a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./tok_dir/tokenizer.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_path = \"./data/wiki_article_en\"\n",
    "tokenizer_dir = \"./tok_dir/\"\n",
    "model_output_dir = \"./model_ckpts/\"\n",
    "\n",
    "tokenizer = BERTWordPieceTokenizer(lowercase=False)\n",
    "tokenizer.train(corpus_path, vocab_size=1000)\n",
    "tokenizer.save(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll initialize a BERT MLM class, passing in saved tokenizer path and setting model hyperparameters in the initialization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/.pyenv/versions/3.6.12/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "model = BERTForMaskedLanguageModeling(tokenizer_dir, hidden_size=252, intermediate_size=256, num_hidden_layers=2)\n",
    "\n",
    "# optionally, users can use a pretrained tokenizer provided by Huggingface, for example:\n",
    "# model = BERTForMaskedLanguageModeling(\"bert-base-cased\", hidden_size=252, intermediate_size=256, num_hidden_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train it on the same corpus. There are a few ways to call the train function. The simplest is to pass the raw data as an argument and pass training arguments as keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 00:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6.547000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>6.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>6.298200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train(train_text=corpus_path, output_dir=model_output_dir, num_train_epochs=5, learning_rate=0.00005, \n",
    "            max_len=256, logging_steps=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, users can pass instantiated torch.Dataset class(es) instead of the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 00:17, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.276600</td>\n",
       "      <td>6.284849</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>102.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.147700</td>\n",
       "      <td>6.228829</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>107.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.101800</td>\n",
       "      <td>6.242190</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>114.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.100700</td>\n",
       "      <td>6.324665</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>99.513000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mangoes.modeling import MangoesLineByLineDataset\n",
    "\n",
    "eval_corpus_path = \"./data/wiki_article_fr\"\n",
    "train_dataset = MangoesLineByLineDataset(corpus_path, model.tokenizer, max_len=256)\n",
    "eval_dataset = MangoesLineByLineDataset(eval_corpus_path, model.tokenizer, max_len=256)\n",
    "\n",
    "model.train(train_dataset=train_dataset, eval_dataset=eval_dataset, output_dir=model_output_dir, \n",
    "            num_train_epochs=4, learning_rate=0.00005, logging_steps=40, evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to instantiate a transformers.Trainer and pass this to the train() function. This is shown in the fine tuning demos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the BERT mode is pretrained, we can use it to get embeddings, or to predict masked tokens. As shown in the feature extraction demo, users can use the predict or generate_outputs functions. In this case, predict gives a direct prediction for the masked token prediction task, while generate outputs gives the masked token scores as well as embeddings or attention matrices, if asked for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'An important current within anarchism is free the.', 'score': 0.03034188225865364, 'token': 160, 'token_str': 'the'}]\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"An important current within anarchism is free {model.tokenizer.mask_token} .\"\n",
    "\n",
    "predictions = model.predict(input_text, top_k=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['logits', 'hidden_states', 'attentions', 'offset_mappings'])\n",
      "torch.Size([1, 10, 1000])\n",
      "tensor(3.0384)\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate_outputs(input_text, output_hidden_states=True, output_attentions=True)\n",
    "print(outputs.keys())\n",
    "print(outputs[\"logits\"].shape)\n",
    "print(outputs[\"logits\"][0][-2][model.tokenizer.convert_tokens_to_ids(\"the\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0303)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "logits = F.softmax(outputs[\"logits\"], dim=-1)\n",
    "print(logits[0][-2][model.tokenizer.convert_tokens_to_ids(\"the\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, it can be saved using the save() function. This is useful to further fine tune the model for a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model_output/\", save_tokenizer=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
