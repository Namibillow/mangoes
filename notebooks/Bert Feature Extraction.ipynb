{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from mangoes.modeling import PretrainedTransformerModelForFeatureExtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load a pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/.pyenv/versions/3.6.12/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = PretrainedTransformerModelForFeatureExtraction.load(\"bert-base-uncased\", \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the text we would like to extract features for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I'm a test sentence.\", \"This is another test sentence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the predict function to obtain the last hidden state from the bert model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "9\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "outputs = pretrained_model.predict(text)\n",
    "print(len(outputs))           # one list of hidden layer outputs per input sentences\n",
    "print(len(outputs[0]))        # sequence length\n",
    "print(len(outputs[0][-1]))    # size of hidden state of last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the same output plus all the previous layer's hidden states, and the attention matrices if needed, using the generate_outputs function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['last_hidden_state', 'pooler_output', 'hidden_states', 'attentions', 'offset_mappings'])\n"
     ]
    }
   ],
   "source": [
    "outputs = pretrained_model.generate_outputs(text, output_hidden_states=True, output_attentions=True)\n",
    "print(outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 768])\n",
      "torch.Size([2, 12, 9, 9])\n",
      "torch.Size([2, 9, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs[\"hidden_states\"][-1].shape)  # batch_size, max_sequence_length, hidden_size\n",
    "print(outputs[\"attentions\"][-1].shape)     # batch_size, num_attention_heads, max_sequence_length, max_sequence_length\n",
    "print(outputs[\"offset_mappings\"].shape)    # batch_size, max_sequence_length, 2:(start and end indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pair hidden states with subtokens using the offset mappings that are part of the output. For example, say we want to create a list of (string, 5th hidden state) tuples for the first sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I : (768,)\n",
      "' : (768,)\n",
      "m : (768,)\n",
      "a : (768,)\n",
      "test : (768,)\n",
      "sentence : (768,)\n",
      ". : (768,)\n"
     ]
    }
   ],
   "source": [
    "string_representations = []\n",
    "for i in range(len(outputs[\"offset_mappings\"][0])):\n",
    "    start, end = outputs[\"offset_mappings\"][0][i]\n",
    "    if not start == end:   # skip special tokens\n",
    "        string = text[0][start:end]\n",
    "        string_representations.append((string, outputs[\"hidden_states\"][5][0][i]))\n",
    "    \n",
    "for (string, hs) in string_representations:\n",
    "    print(f\"{string} : {hs.numpy().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, users want full word embeddings instead of subword token embeddings. The generate outputs method provides functionality for averaging subword tokens into word embeddings. Just set the \"word_embeddings\" argument to True. Here's an example where we get the word embeddings for each word in the second sentence, averaging sub token embeddings if there are multiple in the same word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([5, 768])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "outputs = pretrained_model.generate_outputs(text, output_hidden_states=True, word_embeddings=True)\n",
    "words = text[0].split()\n",
    "print(len(words))\n",
    "print(outputs[\"hidden_states\"][-1][0].shape)\n",
    "print(outputs[\"hidden_states\"][-1][0][-1][:10]) # padded extra word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime users only have access to pre-split text, meaning text that has already been split on whitespace or punctuation. The generate outputs function can handle this data as well, just set the pretokenized argument to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'm a test sentence.'], ['This is another test sentence']]\n",
      "2\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "split_text = [sentence.split(\"'\") for sentence in text]\n",
    "print(split_text)\n",
    "outputs = pretrained_model.generate_outputs(split_text, pre_tokenized=True,\n",
    "                                            output_hidden_states=True, word_embeddings=True)\n",
    "print(len(split_text[0]))\n",
    "print(outputs[\"hidden_states\"][-1][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the PretrainedTransformerModelForFeatureExtraction class works for many other Huggingface model architectures besides BERT, such as ALBERT, which adds parameter sharing to the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "pretrained_albert = PretrainedTransformerModelForFeatureExtraction.load(\"albert-base-v1\", \"albert-base-v1\")\n",
    "\n",
    "outputs = pretrained_albert.generate_outputs(text[1], output_hidden_states=True, word_embeddings=True)\n",
    "print(len(split_text[1]))\n",
    "print(outputs[\"hidden_states\"][-1][0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
